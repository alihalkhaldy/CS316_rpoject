# -*- coding: utf-8 -*-
"""Copy of try 12

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UQbWJtlgLxkmxZUI-sPcCFVWKL3dvwdP
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
mrigaankjaswal_crop_yield_prediction_dataset_path = kagglehub.dataset_download('mrigaankjaswal/crop-yield-prediction-dataset')

print('Data source import complete.')

!pip install scikeras catboost

# Commented out IPython magic to ensure Python compatibility.
# Import necessary libraries
import pandas as pd
import numpy as np
import tensorflow as tf
from scikeras.wrappers import KerasRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import (
    train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, GroupKFold
)
from sklearn.metrics import (
    r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
)
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.feature_selection import mutual_info_regression
import matplotlib.pyplot as plt
import seaborn as sns
from xgboost import XGBRegressor
from catboost import CatBoostRegressor
import time  # For tracking training time

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Ensure plots are displayed in the notebook
# %matplotlib inline

# 1. Problem Definition
def document_problem():
    """
    Document the problem definition and SDG alignment.
    """
    print("Problem Definition:")
    print("RQ1: How does average rainfall affect crop yield?")
    print("RQ2: What is the relationship between pesticide use and crop yield?")
    print("RQ3: Can we predict crop yield accurately using climate and agricultural data?")
    print("\nAlignment with SDGs:")
    print("SDG 2 (Zero Hunger): Improving crop yield predictions enhances food security.")
    print("SDG 12 (Responsible Consumption): Promotes efficient resource usage.")
    print("SDG 13 (Climate Action): Mitigates climate impact by understanding yield-climate relationships.")

document_problem()
dataset_path='/kaggle/input/crop-yield-prediction-dataset/yield_df.csv'
# 2. Data Collection
def load_dataset(dataset_path):
    """
    Load the dataset and perform initial assessment.
    """
    df = pd.read_csv('/kaggle/input/crop-yield-prediction-dataset/yield_df.csv')
    print("\nDataset Source Information:")
    print("Dataset Name: Crop Yield Prediction Dataset")
    print("Source: https://www.kaggle.com/datasets/mrigaankjaswal/crop-yield-prediction-dataset")
    print("Time Period: 1990–2013")
    print("Features: Includes average rainfall, temperature, pesticides used, and crop yield.")
    print("\nDataset Shape:", df.shape)
    print("\nDataset Info:")
    df.info()
    print("\nMissing Values:")
    print(df.isnull().sum())
    return df

# Replace the dataset path with your actual dataset path
dataset_path = '/kaggle/input/crop-yield-prediction-dataset/yield_df.csv'
yield_data = load_dataset(dataset_path)

# 3. Data Preparation
# Data Cleaning
yield_data_cleaned = yield_data.drop(columns=['Unnamed: 0'], errors='ignore').dropna()

# Reset index after dropping rows
yield_data_cleaned.reset_index(drop=True, inplace=True)

# 4. Exploratory Data Analysis (EDA)
def perform_eda(df):
    """
    Conduct exploratory data analysis.
    """
    # Descriptive Statistics
    display(df.describe())

    # Target Variable Distribution
    plt.figure(figsize=(10, 6))
    sns.histplot(df['hg/ha_yield'], kde=True, bins=30)
    plt.title('Distribution of Crop Yield', fontsize=14)
    plt.xlabel('Yield (hg/ha)', fontsize=12)
    plt.ylabel('Frequency', fontsize=12)
    plt.show()

    # Correlation Heatmap
    plt.figure(figsize=(10, 8))
    correlation_matrix = df.corr(numeric_only=True)
    sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="viridis")
    plt.title('Correlation Heatmap', fontsize=14)
    plt.show()

    # Pairplot
    sns.pairplot(df[['average_rain_fall_mm_per_year', 'avg_temp', 'pesticides_tonnes', 'hg/ha_yield']])
    plt.show()

    # Categorical Variable Analysis
    categorical_cols = ['Area', 'Item']
    for col in categorical_cols:
        plt.figure(figsize=(12, 6))
        df[col].value_counts().head(20).plot(kind='bar')
        plt.title(f'Top 20 {col} Categories', fontsize=14)
        plt.xlabel(col, fontsize=12)
        plt.ylabel('Count', fontsize=12)
        plt.show()

perform_eda(yield_data_cleaned)

# 5. Feature Engineering
def engineer_features(df):
    """
    Create and select features without using the target variable.
    """
    df_engineered = df.copy()
    # Interaction Features
    df_engineered['rainfall_times_pesticides'] = (
        df_engineered['average_rain_fall_mm_per_year'] * df_engineered['pesticides_tonnes']
    )
    df_engineered['temp_times_pesticides'] = df_engineered['avg_temp'] * df_engineered['pesticides_tonnes']
    df_engineered['rainfall_times_temp'] = (
        df_engineered['average_rain_fall_mm_per_year'] * df_engineered['avg_temp']
    )
    # Polynomial Features
    for col in ['average_rain_fall_mm_per_year', 'avg_temp', 'pesticides_tonnes']:
        df_engineered[f'{col}^2'] = df_engineered[col] ** 2
        df_engineered[f'{col}^3'] = df_engineered[col] ** 3
    return df_engineered

# Apply feature engineering to the entire dataset
yield_data_fe = engineer_features(yield_data_cleaned)

# 6. Data Preprocessing
# 6.1 Encoding Categorical Variables
def label_encode_features(df, categorical_cols):
    """
    Label encode categorical features.
    """
    df_encoded = df.copy()
    label_encoders = {}
    for col in categorical_cols:
        le = LabelEncoder()
        df_encoded[col] = le.fit_transform(df_encoded[col])
        label_encoders[col] = le
    return df_encoded, label_encoders

categorical_cols = ['Area', 'Item', 'Year']
yield_data_encoded, label_encoders = label_encode_features(yield_data_fe, categorical_cols)

# 6.2 Feature Selection
def select_features(X, y, num_features=50):
    """
    Select top features based on mutual information.
    """
    mi = mutual_info_regression(X, y)
    mi = pd.Series(mi, index=X.columns)
    mi.sort_values(ascending=False, inplace=True)
    print("\nTop Features based on Mutual Information:")
    print(mi.head(num_features))
    selected_features = mi.index[:num_features].tolist()
    return selected_features

# Separate features and target
X = yield_data_encoded.drop(columns=['hg/ha_yield'])
y = yield_data_encoded['hg/ha_yield']

selected_features = select_features(X, y, num_features=50)
X_selected = X[selected_features]

# 6.3 Data Splitting
# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X_selected, y, test_size=0.2, random_state=42, stratify=yield_data_encoded['Area']
)

# 6.4 Scaling Numerical Features
# Identify numerical columns (excluding categorical ones)
numerical_cols = [col for col in X_train.columns if col not in categorical_cols]

# Initialize scaler
scaler = StandardScaler()

# Fit and transform training data
X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])

# Transform test data
X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])

# 7. Modeling
# 7.1 Model Training
def train_models(X_train, y_train):
    """
    Train and compare different models.
    """
    # Define models
    models = {
        'Linear Regression': LinearRegression(),
        'Ridge Regression': Ridge(),
        'Lasso Regression': Lasso(),
        'Random Forest': RandomForestRegressor(random_state=42),
        'Gradient Boosting': GradientBoostingRegressor(random_state=42),
        'XGBoost': XGBRegressor(random_state=42, objective='reg:squarederror'),
        'CatBoost': CatBoostRegressor(random_seed=42, verbose=0),
    }

    best_models = {}
    cv_results = {}
    training_times = {}

    for name, model in models.items():
        print(f"\nTraining {name}...")
        start_time = time.time()  # Start timing

        # Hyperparameter tuning (if applicable)
        if name == 'Random Forest':
            param_grid = {
                'n_estimators': [100, 200],
                'max_depth': [10, 20, None],
                'min_samples_split': [2, 5],
                'min_samples_leaf': [1, 2]
            }
            grid_search = RandomizedSearchCV(
                model, param_distributions=param_grid, n_iter=10,
                cv=3, scoring='r2', n_jobs=-1, random_state=42
            )
            grid_search.fit(X_train, y_train)
            best_models[name] = grid_search.best_estimator_
            cv_results[name] = {
                "Best Parameters": grid_search.best_params_,
                "Best Cross-Validation R²": grid_search.best_score_
            }
        elif name == 'CatBoost':
            # CatBoost handles categorical features natively
            categorical_features_indices = [
                X_train.columns.get_loc(col) for col in categorical_cols if col in X_train.columns
            ]
            model.set_params(cat_features=categorical_features_indices)
            model.fit(X_train, y_train, eval_set=(X_train, y_train), verbose=False)
            best_models[name] = model
            cv_scores = cross_val_score(
                model, X_train, y_train, cv=3, scoring='r2', n_jobs=-1
            )
            cv_results[name] = {
                "Best Cross-Validation R²": cv_scores.mean(),
                "Standard Deviation R²": cv_scores.std()
            }
        else:
            model.fit(X_train, y_train)
            best_models[name] = model
            cv_scores = cross_val_score(
                model, X_train, y_train, cv=3, scoring='r2', n_jobs=-1
            )
            cv_results[name] = {
                "Best Cross-Validation R²": cv_scores.mean(),
                "Standard Deviation R²": cv_scores.std()
            }

        end_time = time.time()  # End timing
        training_times[name] = end_time - start_time  # Record training time

    return best_models, cv_results, training_times

# Train models
best_models, cv_results, training_times = train_models(X_train, y_train)

# 8. Evaluation
# 8.1 Model Evaluation
def evaluate_models(models, X_train, y_train, X_test, y_test):
    """
    Evaluate each model and visualize performance.
    """
    for name, model in models.items():
        print(f"\nEvaluating {name}...")
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)

        # Metrics for Training Data
        r2_train = r2_score(y_train, y_pred_train)
        mse_train = mean_squared_error(y_train, y_pred_train)
        mae_train = mean_absolute_error(y_train, y_pred_train)
        mape_train = mean_absolute_percentage_error(y_train, y_pred_train)

        # Metrics for Test Data
        r2_test = r2_score(y_test, y_pred_test)
        mse_test = mean_squared_error(y_test, y_pred_test)
        mae_test = mean_absolute_error(y_test, y_pred_test)
        mape_test = mean_absolute_percentage_error(y_test, y_pred_test)

        generalization_percentage = (r2_test / r2_train) * 100 if r2_train != 0 else 0

        print(f"R² Score (Train): {r2_train:.4f}")
        print(f"R² Score (Test): {r2_test:.4f}")
        print(f"Generalization Percentage: {generalization_percentage:.2f}%")
        print(f"MSE (Train): {mse_train:.4f} | MSE (Test): {mse_test:.4f}")
        print(f"MAE (Train): {mae_train:.4f} | MAE (Test): {mae_test:.4f}")
        print(f"MAPE (Train): {mape_train:.4f} | MAPE (Test): {mape_test:.4f}")

        # Prediction vs Actual Plot
        plt.figure(figsize=(10, 6))
        plt.scatter(y_test, y_pred_test, alpha=0.5, edgecolor='k')
        plt.title(f'{name}: Actual vs. Predicted (Test Set)', fontsize=14)
        plt.xlabel('Actual Values', fontsize=12)
        plt.ylabel('Predicted Values', fontsize=12)
        plt.plot(
            [y_test.min(), y_test.max()],
            [y_test.min(), y_test.max()],
            color='red', linestyle='--'
        )
        plt.show()

        # Residuals Plot
        residuals = y_test - y_pred_test
        plt.figure(figsize=(10, 6))
        sns.histplot(residuals, kde=True, bins=30, alpha=0.7)
        plt.title(f'{name}: Residual Distribution (Test Set)', fontsize=14)
        plt.xlabel('Residuals', fontsize=12)
        plt.ylabel('Frequency', fontsize=12)
        plt.grid()
        plt.show()

# Evaluate models
evaluate_models(best_models, X_train, y_train, X_test, y_test)

# 8.2 Model Comparison and Selection
def compare_models(models, X_train, y_train, X_test, y_test, cv_results, training_times):
    """
    Compare all models and generate comparative visualizations.
    """
    metrics = {}
    for name, model in models.items():
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)

        # Metrics
        r2_train = r2_score(y_train, y_pred_train)
        r2_test = r2_score(y_test, y_pred_test)
        mse_train = mean_squared_error(y_train, y_pred_train)
        mae_train = mean_absolute_error(y_train, y_pred_train)
        mape_train = mean_absolute_percentage_error(y_train, y_pred_train)
        mse_test = mean_squared_error(y_test, y_pred_test)
        mae_test = mean_absolute_error(y_test, y_pred_test)
        mape_test = mean_absolute_percentage_error(y_test, y_pred_test)
        generalization_percentage = (r2_test / r2_train) * 100 if r2_train != 0 else 0

        metrics[name] = {
            'R² (Train)': r2_train,
            'R² (Test)': r2_test,
            'MSE (Train)': mse_train,
            'MSE (Test)': mse_test,
            'MAE (Train)': mae_train,
            'MAE (Test)': mae_test,
            'MAPE (Train)': mape_train,
            'MAPE (Test)': mape_test,
            'Generalization %': generalization_percentage
        }

        print(f"{name:<25}R² (Train): {r2_train:.4f} | R² (Test): {r2_test:.4f}")

    # Create metrics DataFrame for visualizations
    metrics_df = pd.DataFrame(metrics).T

    # Performance Comparison
    metrics_df[['R² (Train)', 'R² (Test)']].plot(kind='bar', figsize=(12, 6), rot=45)
    plt.title('Model Performance Comparison: R² Scores')
    plt.ylabel('R² Score')
    plt.grid(axis='y')
    plt.show()

    # Generalization Comparison
    metrics_df['Generalization %'].plot(kind='bar', figsize=(12, 6), color='green', rot=45)
    plt.title('Generalization Percentage Comparison')
    plt.ylabel('Generalization Percentage (%)')
    plt.grid(axis='y')
    plt.show()

    # MSE Comparison
    metrics_df[['MSE (Train)', 'MSE (Test)']].plot(kind='bar', figsize=(12, 6), rot=45)
    plt.title('Model Performance Comparison: MSE')
    plt.ylabel('Mean Squared Error')
    plt.grid(axis='y')
    plt.show()

    # MAE Comparison
    metrics_df[['MAE (Train)', 'MAE (Test)']].plot(kind='bar', figsize=(12, 6), rot=45)
    plt.title('Model Performance Comparison: MAE')
    plt.ylabel('Mean Absolute Error')
    plt.grid(axis='y')
    plt.show()

    # MAPE Comparison
    metrics_df[['MAPE (Train)', 'MAPE (Test)']].plot(kind='bar', figsize=(12, 6), rot=45)
    plt.title('Model Performance Comparison: MAPE')
    plt.ylabel('Mean Absolute Percentage Error')
    plt.grid(axis='y')
    plt.show()

    # Cross-Validation Performance
    cv_df = pd.DataFrame({
        'Model': list(cv_results.keys()),
        'Best CV R²': [cv_results[model]['Best Cross-Validation R²'] for model in cv_results]
    })
    cv_df.set_index('Model').plot(kind='bar', figsize=(12, 6), color='skyblue', rot=45)
    plt.title('Cross-Validation Performance Comparison')
    plt.ylabel('R² Score')
    plt.grid(axis='y')
    plt.show()

    # Training Time Comparison
    time_df = pd.DataFrame({
        'Model': list(training_times.keys()),
        'Training Time (s)': list(training_times.values())
    })
    time_df.set_index('Model')['Training Time (s)'].plot(kind='bar', figsize=(12, 6), color='orange', rot=45)
    plt.title('Training Time Comparison')
    plt.ylabel('Time (seconds)')
    plt.grid(axis='y')
    plt.show()

# Compare models
compare_models(best_models, X_train, y_train, X_test, y_test, cv_results, training_times)

# 9. Feature Importance Analysis
def plot_feature_importance(model, X_train):
    """
    Plot feature importances for models that support it.
    """
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
        feature_importance_df = pd.DataFrame({
            'Feature': X_train.columns,
            'Importance': importances
        })
        feature_importance_df.sort_values(by='Importance', ascending=False, inplace=True)
        plt.figure(figsize=(10, 6))
        sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(20))
        plt.title(f'Feature Importances for Model: {model.__class__.__name__}')
        plt.show()

# Plot feature importances for CatBoost model (or any other with feature_importances_)
best_model_name = 'CatBoost'
best_model = best_models[best_model_name]
plot_feature_importance(best_model, X_train)